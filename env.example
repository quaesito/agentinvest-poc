# AgentInvest PoC Environment Configuration
# Copy this file to .env and fill in your actual API keys

# ===== REQUIRED API KEYS =====

# OpenRouter API Key (Required)
# Get your API key from: https://openrouter.ai/keys
# Supports multiple LLM providers: Gemini, GPT, Claude, Llama, etc.
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Tavily API Key (Required) 
# Get your API key from: https://tavily.com
# Used for web search and content extraction
TAVILY_API_KEY=your_tavily_api_key_here

# ===== OPTIONAL CONFIGURATIONS =====

# Chart.js Library Source (Optional)
# Default: https://cdn.jsdelivr.net/npm/chart.js
# Use local file if you have chart.min.js mounted in the container
# CHARTJS_SRC=https://cdn.jsdelivr.net/npm/chart.js
# CHARTJS_SRC=/app/vendor/chart.min.js

# Redis Configuration (Optional)
# Default: localhost:6379 for local development
# For Docker: redis://redis:6379
# REDIS_URL=redis://localhost:6379

# ===== MODEL CONFIGURATION EXAMPLES =====
# You can modify these in agent.py to use different models

# Current default models (configured in agent.py):
# PRIMARY_MODEL=google/gemini-2.0-flash-001
# SECONDARY_MODEL=google/gemini-2.5-flash

# Alternative model options through OpenRouter:
# - OpenAI: openai/gpt-4o, openai/gpt-4o-mini
# - Anthropic: anthropic/claude-3.5-sonnet, anthropic/claude-3-haiku  
# - Meta: meta-llama/llama-3.1-70b-instruct, meta-llama/llama-3.1-8b-instruct
# - Google: google/gemini-2.0-flash-001, google/gemini-pro
# - See full list at: https://openrouter.ai/models

# ===== DOCKER ENVIRONMENT NOTES =====
# When running in Docker:
# - Redis URL should be: redis://redis:6379
# - Generated reports will be saved to /app/generated_reports/
# - Mount this directory as a volume to persist reports on host

# ===== DEVELOPMENT NOTES =====
# For local development without Docker:
# - Install wkhtmltopdf: sudo apt-get install wkhtmltopdf (Linux) or brew install wkhtmltopdf (macOS)
# - Install Redis: sudo apt-get install redis-server (Linux) or brew install redis (macOS)
# - Run Redis: redis-server
# - Run app: streamlit run streamlit_app.py
